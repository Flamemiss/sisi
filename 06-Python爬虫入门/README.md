# ä»»åŠ¡6ï¼šPythonçˆ¬è™«å…¥é—¨

## ğŸ¯ ä»»åŠ¡ç›®æ ‡

å­¦ä¼šç”¨Pythonè·å–ç½‘é¡µæ•°æ®ï¼Œå¹¶æå–æœ‰ç”¨ä¿¡æ¯ä¿å­˜ä¸‹æ¥ã€‚

---

## ğŸ“‹ éªŒæ”¶æ ‡å‡†

| åºå· | éªŒæ”¶é¡¹ç›® | å…·ä½“è¦æ±‚ |
|------|----------|----------|
| 1 | åº“å®‰è£… | æˆåŠŸå®‰è£… requests å’Œ beautifulsoup4 |
| 2 | è·å–ç½‘é¡µ | ç”¨requestsè·å–ç½‘é¡µHTMLå†…å®¹ |
| 3 | è§£æHTML | ç”¨BeautifulSoupæå–æ ‡é¢˜ã€é“¾æ¥ |
| 4 | æ•°æ®ä¿å­˜ | å°†çˆ¬å–æ•°æ®ä¿å­˜ä¸ºæ–‡ä»¶ |
| 5 | å¼‚å¸¸å¤„ç† | èƒ½å¤„ç†ç½‘ç»œé”™è¯¯æƒ…å†µ |

---

## âš ï¸ é‡è¦æé†’

**çˆ¬è™«é“å¾·ä¸æ³•å¾‹**ï¼š

1. éµå®ˆç½‘ç«™çš„ `robots.txt` è§„åˆ™
2. æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œä¸è¦ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
3. ä»…ç”¨äºå­¦ä¹ ç›®çš„ï¼Œä¸çˆ¬å–æ•æ„Ÿä¿¡æ¯
4. éƒ¨åˆ†ç½‘ç«™ç¦æ­¢çˆ¬è™«ï¼Œè¯·éµå®ˆç½‘ç«™è§„å®š

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

```cmd
pip install requests beautifulsoup4 -i https://pypi.tuna.tsinghua.edu.cn/simple
```

---

## ğŸ“ ç»ƒä¹ ä»»åŠ¡

### ç»ƒä¹ 1ï¼šè·å–ç½‘é¡µå†…å®¹

```python
# fetch_page.py
import requests

# ç›®æ ‡ç½‘å€ï¼ˆç™¾åº¦ä¸ºä¾‹ï¼‰
url = "https://www.baidu.com"

# å‘é€è¯·æ±‚
response = requests.get(url)

# æŸ¥çœ‹çŠ¶æ€ç ï¼ˆ200è¡¨ç¤ºæˆåŠŸï¼‰
print(f"çŠ¶æ€ç : {response.status_code}")

# æŸ¥çœ‹ç½‘é¡µå†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰
print(f"\nç½‘é¡µå†…å®¹é¢„è§ˆ:\n{response.text[:500]}")
```

### ç»ƒä¹ 2ï¼šæ·»åŠ è¯·æ±‚å¤´

æŸäº›ç½‘ç«™ä¼šæ£€æµ‹è¯·æ±‚æ¥æºï¼Œéœ€è¦æ·»åŠ è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨ï¼š

```python
# fetch_with_headers.py
import requests

url = "https://www.baidu.com"

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

response = requests.get(url, headers=headers)
print(f"çŠ¶æ€ç : {response.status_code}")
```

### ç»ƒä¹ 3ï¼šè§£æHTML

```python
# parse_html.py
from bs4 import BeautifulSoup
import requests

url = "https://www.baidu.com"
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

response = requests.get(url, headers=headers)
response.encoding = 'utf-8'

# åˆ›å»ºBeautifulSoupå¯¹è±¡
soup = BeautifulSoup(response.text, 'html.parser')

# è·å–é¡µé¢æ ‡é¢˜
title = soup.title.string
print(f"é¡µé¢æ ‡é¢˜: {title}")

# è·å–æ‰€æœ‰é“¾æ¥
print("\né¡µé¢ä¸­çš„é“¾æ¥:")
for link in soup.find_all('a')[:10]:  # åªå–å‰10ä¸ª
    href = link.get('href')
    text = link.get_text().strip()
    if href and text:
        print(f"  {text}: {href}")
```

### ç»ƒä¹ 4ï¼šçˆ¬å–å¤©æ°”ä¿¡æ¯

```python
# weather_crawler.py
import requests
from bs4 import BeautifulSoup

def get_weather():
    """çˆ¬å–å¤©æ°”ä¿¡æ¯ï¼ˆç¤ºä¾‹ç”¨ä¸­å›½å¤©æ°”ç½‘ï¼‰"""
    
    # åŒ—äº¬å¤©æ°”é¡µé¢
    url = "http://www.weather.com.cn/weather1d/101010100.shtml"
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å°è¯•è·å–å¤©æ°”ä¿¡æ¯
        print("=== å¤©æ°”é¡µé¢çˆ¬å–æˆåŠŸ ===")
        print(f"é¡µé¢æ ‡é¢˜: {soup.title.string}")
        
        # æ³¨æ„ï¼šå®é™…ç½‘é¡µç»“æ„å¯èƒ½å˜åŒ–ï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é€‰æ‹©å™¨
        
    except requests.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")

if __name__ == "__main__":
    get_weather()
```

### ç»ƒä¹ 5ï¼šçˆ¬å–å¹¶ä¿å­˜æ•°æ®

```python
# save_data.py
import requests
from bs4 import BeautifulSoup
import json
import csv

def crawl_and_save():
    """çˆ¬å–æ•°æ®å¹¶ä¿å­˜ä¸ºå¤šç§æ ¼å¼"""
    
    # ç¤ºä¾‹ï¼šçˆ¬å–ä¸€ä¸ªç®€å•é¡µé¢
    url = "https://www.example.com"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ•°æ®
        data = {
            "title": soup.title.string if soup.title else "æ— æ ‡é¢˜",
            "url": url,
            "paragraphs": [p.get_text().strip() for p in soup.find_all('p')]
        }
        
        # ä¿å­˜ä¸ºJSON
        with open('result.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print("å·²ä¿å­˜åˆ° result.json")
        
        # ä¿å­˜ä¸ºæ–‡æœ¬
        with open('result.txt', 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {data['title']}\n")
            f.write(f"ç½‘å€: {data['url']}\n")
            f.write("\næ®µè½å†…å®¹:\n")
            for i, p in enumerate(data['paragraphs'], 1):
                f.write(f"{i}. {p}\n")
        print("å·²ä¿å­˜åˆ° result.txt")
        
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    crawl_and_save()
```

### ç»ƒä¹ 6ï¼šå®Œæ•´çˆ¬è™«æ¨¡æ¿

```python
# crawler_template.py
"""
é€šç”¨çˆ¬è™«æ¨¡æ¿
"""
import requests
from bs4 import BeautifulSoup
import time
import json

class SimpleCrawler:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        self.session = requests.Session()
    
    def fetch(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = self.session.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            response.encoding = response.apparent_encoding  # è‡ªåŠ¨æ£€æµ‹ç¼–ç 
            return response.text
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥ [{url}]: {e}")
            return None
    
    def parse(self, html):
        """è§£æHTMLï¼ˆå­ç±»é‡å†™æ­¤æ–¹æ³•ï¼‰"""
        soup = BeautifulSoup(html, 'html.parser')
        return {
            "title": soup.title.string if soup.title else None,
            "links": [a.get('href') for a in soup.find_all('a', href=True)]
        }
    
    def save(self, data, filename):
        """ä¿å­˜æ•°æ®"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    def run(self, url):
        """è¿è¡Œçˆ¬è™«"""
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        html = self.fetch(url)
        if html:
            data = self.parse(html)
            self.save(data, 'output.json')
            return data
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    crawler = SimpleCrawler()
    result = crawler.run("https://www.example.com")
    if result:
        print(f"çˆ¬å–æˆåŠŸï¼è·å–åˆ° {len(result.get('links', []))} ä¸ªé“¾æ¥")
```

---

## ğŸ”§ å¸¸ç”¨æ–¹æ³•é€ŸæŸ¥

### Requests

| æ–¹æ³• | ä½œç”¨ |
|------|------|
| `requests.get(url)` | GETè¯·æ±‚ |
| `response.text` | è·å–æ–‡æœ¬å†…å®¹ |
| `response.json()` | è§£æJSON |
| `response.status_code` | çŠ¶æ€ç  |

### BeautifulSoup

| æ–¹æ³• | ä½œç”¨ |
|------|------|
| `soup.find('tag')` | æ‰¾ç¬¬ä¸€ä¸ªæ ‡ç­¾ |
| `soup.find_all('tag')` | æ‰¾æ‰€æœ‰æ ‡ç­¾ |
| `soup.select('cssé€‰æ‹©å™¨')` | CSSé€‰æ‹©å™¨ |
| `tag.get_text()` | è·å–æ–‡æœ¬ |
| `tag.get('å±æ€§')` | è·å–å±æ€§å€¼ |

---

## âœ… è‡ªæµ‹æ¸…å•

- [ ] çŸ¥é“ `requests.get()` å¦‚ä½•å‘é€è¯·æ±‚å—ï¼Ÿ
- [ ] çŸ¥é“ä¸ºä»€ä¹ˆè¦æ·»åŠ  User-Agent è¯·æ±‚å¤´å—ï¼Ÿ
- [ ] èƒ½ç”¨ BeautifulSoup æå–ç½‘é¡µæ ‡é¢˜å—ï¼Ÿ
- [ ] çŸ¥é“ `find()` å’Œ `find_all()` çš„åŒºåˆ«å—ï¼Ÿ
- [ ] èƒ½æŠŠçˆ¬å–çš„æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶å—ï¼Ÿ

---

## ğŸ“š æ¨èèµ„æº

- Requestsæ–‡æ¡£ï¼šhttps://docs.python-requests.org/zh_CN/latest/
- BeautifulSoupæ–‡æ¡£ï¼šhttps://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/
- å´”åº†æ‰çˆ¬è™«æ•™ç¨‹ï¼šhttps://cuiqingcai.com/
